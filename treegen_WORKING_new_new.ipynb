{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log2\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#to use for testing, amend path to txt file\n",
    "#===========================================#\n",
    "\n",
    "clean_data = np.loadtxt(\"clean_dataset.txt\") \n",
    "noisy_data = np.loadtxt(\"noisy_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data): # input is a multi dimensonal numpy array \n",
    "    num_entries = len(data) # total number of instances in the dataset\n",
    "    outcomes = defaultdict(int) # dict to hold count of each label in dataset\n",
    "    entropy = 0 # initial entropy value\n",
    "    for row in data:\n",
    "        key = row[-1] # label is last value of each instance\n",
    "        outcomes[key] += 1 # increment the count of the label \n",
    "\n",
    "    for key in outcomes:\n",
    "        outcomes[key] /= num_entries # compute the probability of each label \n",
    "        prob_label = outcomes[key] \n",
    "        entropy -= (prob_label*log2(prob_label)) # computing the entropy value\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(All, Left, Right):\n",
    "    \n",
    "    H_SAll=entropy(All) #entropy of parent datset\n",
    "    Total=len(All)\n",
    "    Remainder=((len(Left)/Total)*entropy(Left))+((len(Right)/Total)*entropy(Right)) #weighted entropies of left and right datasets\n",
    "    Gains=H_SAll-Remainder\n",
    "    \n",
    "    return Gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIND_SPLIT(dataset):\n",
    "    emitter = 0\n",
    "    value = 0\n",
    "    max_info_gain = 0\n",
    "    right = []\n",
    "    left = []\n",
    "    for x in range(len(dataset[0])-1): #first 7 entries, exclude room\n",
    "        \n",
    "        ds = np.array(sorted(dataset, key = lambda y: y[x], reverse=True)) #sort n dimensional array by emitter\n",
    "        \n",
    "        for r in range(len(ds)): #iterate over every value for each emitter\n",
    "            \n",
    "            split_point = ds[r][x] #current point to check\n",
    "            if(r!=len(ds)-1 and ds[r+1][x] == split_point): #check for repeated values, use last instance of value as split point\n",
    "                continue\n",
    "            info_gain = InfoGain(ds, ds[:r+1],ds[r+1:]) #calculte information gain for current point\n",
    "            \n",
    "            if(info_gain > max_info_gain): #if information gain is higher, update the best split point and emitter\n",
    "                left = ds[:r+1]\n",
    "                right = ds[r+1:]\n",
    "                emitter = x\n",
    "                value = split_point\n",
    "                max_info_gain = info_gain\n",
    "            \n",
    "            \n",
    "        \n",
    "    #return emitter, value, left, right\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_learning(training_data, depth):\n",
    "    \n",
    "    if(entropy(training_data) == 0 and len(training_data != 0)): #if all room labels are the same, return leaf which contains room number\n",
    "        count = len(training_data) #counting the number of entries in the dataset where the room labels are all the same (used later for pruning purposes)\n",
    "        label = training_data[0][-1]\n",
    "        leaf = {'emitter':-1, 'value':-1, 'room': label, 'right':-1, 'left':-1, 'count': count, 'leaf': True}\n",
    "        return leaf, depth\n",
    "    \n",
    "    else: \n",
    "        em, val, ldata, rdata = FIND_SPLIT(training_data) #split data to get the child datasets and split point\n",
    "        root = {'emitter': em, 'value': val, 'room': -1, 'right':-1, 'left':-1, 'count': -1, 'leaf': False} #create non leaf node\n",
    "        root['left'], l_depth = tree_learning(ldata, depth+1) #recursively call tree learning to generate rest of tree, increasing depth\n",
    "        root['right'], r_depth = tree_learning(rdata, depth+1)\n",
    "        return root, max(l_depth, r_depth) #return root of tree and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5b7ee93f4b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#===========================================#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtree_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3129209ad49c>\u001b[0m in \u001b[0;36mtree_learning\u001b[0;34m(training_data, depth)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFIND_SPLIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#split data to get the child datasets and split point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'emitter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'room'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'leaf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#create non leaf node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#recursively call tree learning to generate rest of tree, increasing depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "#===========================================#\n",
    "#for testing tree generation only\n",
    "#===========================================#\n",
    "st=time.time()\n",
    "tree_root, depth = tree_learning(clean_data, 0)\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "#print(tree_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_result(row, root):\n",
    "    \n",
    "    if(root['room']!= -1): #check for leaf node, return room\n",
    "        return root['room']\n",
    "    else:\n",
    "        if(row[root['emitter']]>=root['value']): #traverse tree to find leaf node\n",
    "            return return_result(row, root['left'])\n",
    "        else:\n",
    "            return return_result(row, root['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(results_table): \n",
    "    \n",
    "    cm = np.zeros((4,4)) #empty confusion matrix\n",
    "    \n",
    "    for i in range (1,5):\n",
    "        for pair in results_table: #pair[0] is actual value, pair[1] is predicted value by decision tree\n",
    "            if(i == pair[0] and pair[0] == pair[1]): #count correct predictions and update confusion matrix\n",
    "                    cm[i-1][i-1]+=1\n",
    "            if(i == pair[0] and pair[1] != i):\n",
    "                    cm[i-1][int(pair[1])-1]+=1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cm):\n",
    "    if(np.sum(cm) == 0):\n",
    "        return 0\n",
    "    return np.trace(cm)/np.sum(cm) #returns accuracy from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#function returns BOTH confusion matrix and accuracy, different from specification which only required us to return accuracy\n",
    "#===========================================#\n",
    "\n",
    "def evaluate(test, root): \n",
    "    \n",
    "    results_table = np.empty((len(test),2))\n",
    "    \n",
    "    for i in range(len(test)): #iterate over test data and generate prediction\n",
    "        prediction = return_result(test[i], root)\n",
    "        results_table[i] = [test[i][-1],prediction] #keep track of actual outcome vs predicted outcome\n",
    "        \n",
    "    cm = confusion_matrix(results_table) #generate confusion matrix from results table\n",
    "    tree_accuracy = accuracy(cm) #calculate accuracy from confusion matrix\n",
    "    \n",
    "    return cm, tree_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#cross validation WITHOUT pruning\n",
    "#===========================================#\n",
    "\n",
    "def cross_validation(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffles dataset\n",
    "\n",
    "    fold_size = int(len(dataset)/10) \n",
    "    running_cm = np.zeros((4,4)) #generate empty confusion matrix\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        #cycling test and training data for cross valdiation\n",
    "        test = dataset[0+fold_size*i:fold_size*(i+1)] \n",
    "        training = np.vstack((dataset[0:0+fold_size*i],dataset[fold_size*(i+1):])) \n",
    "        \n",
    "        root, depth = tree_learning(training, 0) #generate tree for each cycled training dataset\n",
    "        \n",
    "        confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "        running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "            \n",
    "    avg_cm = running_cm/10 #calculate average confusion matrix\n",
    "    \n",
    "    return avg_cm #return average confusion matrix\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------#\n",
    "testing = np.loadtxt(\"clean_dataset.txt\")\n",
    "st=time.time()\n",
    "print(cross_validation(testing))\n",
    "et=time.time()\n",
    "print('Runtime: ', et-st)\n",
    "#-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(cm): \n",
    "    class_precisions = np.empty(4)\n",
    "    cols = cm.sum(axis=0) \n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        class_precisions[i] = (cm[i][i]/cols[i]) #calculate precision for each class\n",
    "    \n",
    "    return class_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm):\n",
    "    class_recalls = np.empty(4)\n",
    "    rows = cm.sum(axis=1)\n",
    "   \n",
    "    for i in range(len(cm)):\n",
    "        class_recalls[i] = (cm[i][i]/rows[i]) #calculate recall for each class\n",
    "    \n",
    "    return class_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(recall, precision):\n",
    "    class_f1s = np.empty(4)\n",
    "    for i in range(len(recall)):\n",
    "        class_f1s[i] = ((2*recall[i]*precision[i])/(recall[i]+precision[i])) #calculate f1 measure for each class using precision and recall values\n",
    "    \n",
    "    return class_f1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(root, validation_set, start): \n",
    "        \n",
    "        \n",
    "    if ((root['left']['emitter'] == -1) and (root['right']['emitter'] == -1)): #if both children leaf nodes\n",
    "        \n",
    "        old = [root['emitter'], root['value'], root['room'], root['right'], root['left'], root['count']] #store all parent and children before pruning\n",
    "\n",
    "        cm, old_accuracy = evaluate(validation_set, start) #obtain accuracy of tree before prune\n",
    "        \n",
    "        \n",
    "        if(root['left']['count'] > root['right']['count']): #choose majority class label using count value\n",
    "            root['room'] = root['left']['room']\n",
    "            root['count'] = root['left']['count']\n",
    "        else:\n",
    "            root['room'] = root['right']['room']\n",
    "            root['count'] = root['right']['count']\n",
    "\n",
    "        #make parent node into single leaf node\n",
    "        root['left'] = -1 \n",
    "        root['right'] = -1\n",
    "        root['emitter'] = -1\n",
    "        root['value'] = -1\n",
    "        root['leaf'] = True\n",
    "        \n",
    "        \n",
    "        cm, new_accuracy = evaluate(validation_set, start) #obtain accuracy of tree after pruning\n",
    "        \n",
    "        if new_accuracy < old_accuracy: #if pre-pruned accuracy was better, undo prune\n",
    "            root['emitter'] = old[0]\n",
    "            root['value'] = old[1]\n",
    "            root['room'] = old[2]\n",
    "            root['right'] = old[3]\n",
    "            root['left'] = old[4]\n",
    "            root['count'] = old[5]\n",
    "        \n",
    "        return start\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        #traverse tree to find prunable nodes\n",
    "        if(root['left']['emitter'] != -1): \n",
    "            start = prune(root['left'], validation_set, start)\n",
    "\n",
    "        if(root['right']['emitter'] != -1):\n",
    "            start = prune(root['right'], validation_set, start)\n",
    "            \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#cross validation WITH pruning\n",
    "#===========================================#\n",
    "\n",
    "def cross_validation_prune(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffle dataset\n",
    "    fold_size = int(len(dataset)/10)\n",
    "    \n",
    "    #initialise confusion matrices for pre and post pruned trees\n",
    "    running_cm = np.zeros((4,4))\n",
    "    running_cm_pr = np.zeros((4,4))\n",
    "    worse = False\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        #cycle test, training and validation sets\n",
    "        test = dataset[fold_size*i:fold_size*(i+1)]\n",
    "        validation = dataset[fold_size*(i+1)+1:fold_size*(i+2)+1]\n",
    "        training = np.vstack((dataset[0:fold_size*i],dataset[fold_size*(i+2)+1:]))\n",
    "        \n",
    "        #generate decision tree\n",
    "        root, depth = tree_learning(training, 0)\n",
    "                \n",
    "        confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "        running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "        old_acc = tree_accuracy\n",
    "        while(True): #keep pruning until accuracy does not improve\n",
    "            pruned_tree = prune(root, validation, root) #generate pruned tree\n",
    "            x, cur_acc = evaluate(validation, root)\n",
    "            if(cur_acc <= old_acc):\n",
    "                break\n",
    "            old_acc = cur_acc\n",
    "            \n",
    "        #print('Depth after:', findDepth(pruned_tree))\n",
    "        \n",
    "        cm_prune, pr_acc = evaluate(test, pruned_tree) #generate confusion matrix for current pruned tree \n",
    "        running_cm_pr += cm_prune #keep track of confusion matrices to calculate average for pruned tree\n",
    "        \n",
    "        print(depth, \" vs \", findDepth(pruned_tree))\n",
    "        \n",
    "#       print(tree_accuracy, \" vs \", pr_acc)\n",
    "        \n",
    "    \n",
    "    avg_cm = running_cm/10\n",
    "    avg_cm_pr = running_cm_pr/10\n",
    "    \n",
    "#   print(\"Before prune accuracy:\", accuracy(avg_cm))\n",
    "#   print(\"After prune accuracy:\", accuracy(avg_cm_pr))\n",
    "    \n",
    "    return accuracy(avg_cm), accuracy(avg_cm_pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#nested cross validation WITH pruning\n",
    "#===========================================#\n",
    "\n",
    "def nested_cross_validation_prune(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffle dataset\n",
    "    fold_size = int(len(dataset)/10)\n",
    "    \n",
    "    #initialise confusion matrices for pre and post pruned trees\n",
    "    running_cm = np.zeros((4,4))\n",
    "    running_cm_pr = np.zeros((4,4))\n",
    "    \n",
    "    for j in range(10):\n",
    "    \n",
    "        test = dataset[fold_size*j:fold_size*(j+1)]\n",
    "        traning_validation_dataset = np.vstack((dataset[0:fold_size*j], dataset[fold_size*(j+1):]))\n",
    "        \n",
    "        for i in range(9):\n",
    "        \n",
    "            #cycle test, training and validation sets\n",
    "            \n",
    "            validation = traning_validation_dataset[fold_size*(i+1)+1:fold_size*(i+2)+1]\n",
    "            training = np.vstack((traning_validation_dataset[0:fold_size*i],traning_validation_dataset[fold_size*(i+2)+1:]))\n",
    "        \n",
    "            #generate decision tree\n",
    "            root, depth = tree_learning(training, 0)\n",
    "                \n",
    "            confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "            running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "                  \n",
    "            old_acc = tree_accuracy\n",
    "            while(True): #keep pruning until accuracy does not improve\n",
    "                pruned_tree = prune(root, validation, root) #generate pruned tree\n",
    "                x, cur_acc = evaluate(validation, root)\n",
    "                if(cur_acc <= old_acc):\n",
    "                    break\n",
    "                old_acc = cur_acc\n",
    "            #print('Depth after:', findDepth(pruned_tree))\n",
    "        \n",
    "            cm_prune, pr_acc = evaluate(test, pruned_tree) #generate confusion matrix for current pruned tree \n",
    "            running_cm_pr += cm_prune #keep track of confusion matrices to calculate average for pruned tree\n",
    "        \n",
    "            #print(depth, \" vs \", findDepth(pruned_tree))\n",
    "            #print(tree_accuracy, \" vs \", pr_acc)\n",
    "\n",
    "    \n",
    "    avg_cm = running_cm/90\n",
    "    avg_cm_pr = running_cm_pr/90\n",
    "    \n",
    "#   print(\"Before prune accuracy:\", accuracy(avg_cm))\n",
    "#   print(\"After prune accuracy:\", accuracy(avg_cm_pr))\n",
    "    \n",
    "    return accuracy(avg_cm), accuracy(avg_cm_pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------#\n",
    "testing = np.loadtxt(\"clean_dataset.txt\")\n",
    "st=time.time()\n",
    "matrix = cross_validation(testing)\n",
    "print(\"Clean Matrix: \", matrix)\n",
    "acc = accuracy(matrix)\n",
    "pres = precision(matrix)\n",
    "rec = recall(matrix)\n",
    "print(\"Clean accuracy: \", acc)\n",
    "print(\"Clean precision: \", pres)\n",
    "print(\"Clean recall: \", rec)\n",
    "fmeasure = f1(rec, pres)\n",
    "print(\"Clean fmeasure: \", fmeasure )\n",
    "et=time.time()\n",
    "print('Runtime: ', et-st)\n",
    "#-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------#\n",
    "testing = np.loadtxt(\"noisy_dataset.txt\")\n",
    "st=time.time()\n",
    "matrix = cross_validation(testing)\n",
    "print(\"noisy Matrix: \", matrix)\n",
    "acc = accuracy(matrix)\n",
    "pres = precision(matrix)\n",
    "rec = recall(matrix)\n",
    "print(\"noisy accuracy: \", acc)\n",
    "print(\"noisy precision: \", pres)\n",
    "print(\"noisy recall: \", rec)\n",
    "fmeasure = f1(rec, pres)\n",
    "print(\"noisy fmeasure: \", fmeasure )\n",
    "et=time.time()\n",
    "print('Runtime: ', et-st)\n",
    "#-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDepth(node): \n",
    "    \n",
    "    lDepth = 0\n",
    "    rDepth = 0\n",
    "    \n",
    "    if(node['leaf'] ==  True): \n",
    "        return 0\n",
    "  \n",
    "    else : \n",
    "\n",
    "        #find depth for each subtree\n",
    "        lDepth = findDepth(node['left']) \n",
    "        rDepth = findDepth(node['right']) \n",
    "  \n",
    "        #take the larger depth since looking for maximum\n",
    "        if (lDepth > rDepth): \n",
    "            return lDepth+1\n",
    "        else: \n",
    "            return rDepth+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print\n",
    "def countLeaf(tree):\n",
    "    if(tree is None):\n",
    "        return 0\n",
    "    if(tree['left'] == -1 and tree['right'] == -1): \n",
    "        return 1\n",
    "    else:\n",
    "        return countLeaf(tree['left']) + countLeaf(tree['right'])\n",
    "\n",
    "leafNode=dict(boxstyle=\"round4\", fc=(1., .8, 1.))\n",
    "treeNode=dict(boxstyle=\"round4\", fc=\"0.8\") #style of the bubble around the nodes\n",
    "arrows=dict(arrowstyle=\"<-\") #type of arrows \n",
    "\n",
    "##################\n",
    "#function to draw the node bubble with the text\n",
    "##################\n",
    "def draw_node(nodeText,centerPt,parentPt,nodeType):\n",
    "    display_tree.ax1.annotate(nodeText,xy=parentPt,xycoords='axes fraction',xytext=centerPt, va='bottom',ha='center',bbox=nodeType,arrowprops=arrows)\n",
    "\n",
    "def draw_decision(cntrPt, parentPt, decision):\n",
    "    xPt = (parentPt[0]-cntrPt[0])/2.0+cntrPt[0]\n",
    "    yPt = (parentPt[1]-cntrPt[1])/2.0+cntrPt[1]\n",
    "    if(decision is True):\n",
    "        display_tree.ax1.text(xPt, yPt, \"True\")\n",
    "    else:\n",
    "        display_tree.ax1.text(xPt, yPt, \"False\")\n",
    "        \n",
    "def draw_tree(root, parentPt, decision):\n",
    "    if (root is None):\n",
    "        return\n",
    "    leaf_num = countLeaf(root)  \n",
    "    height = findDepth(root)    \n",
    "    cntrPt = (draw_tree.x + (1.0 + float(leaf_num))/2.0/draw_tree.total_width, draw_tree.y) #determines the x and y coordinates for the node\n",
    "    draw_tree.y = (draw_tree.y - 1.0/draw_tree.total_depth)  # y coordinate for the next level of the tree\n",
    "    draw_decision(cntrPt, parentPt, decision)\n",
    "    #if (root['right'] != -1 and root['left'] != -1):  #for non leaf nodes\n",
    "    if(root['leaf'] is False):\n",
    "        textInNode=str('X <' + str(root['value'])+'\\n'+ str(root['emitter']))  #text that is supposed to be in the node\n",
    "        draw_node(textInNode, cntrPt, parentPt, treeNode)\n",
    "        draw_tree(root['left'], cntrPt, False)\n",
    "        draw_tree(root['right'], cntrPt, True)        \n",
    "        \n",
    "    else:   #it's a leaf node print the leaf node\n",
    "        textInNode=str(root['room'])   #text that is supposed to be in the leaf node\n",
    "        draw_tree.x = (draw_tree.x + 1.0/draw_tree.total_width)  # x coordinate for the new node\n",
    "        draw_node(textInNode, cntrPt, parentPt, leafNode) \n",
    "    draw_tree.y = (draw_tree.y + 1.0/draw_tree.total_depth)  # y coord for the next level of the tree\n",
    "\n",
    "def display_tree(root):\n",
    "    fig=plt.figure(1,facecolor='white')  # new figure\n",
    "    \n",
    "    axprops=dict(xticks=[],yticks=[])  #without the x and y major and minor ticks \n",
    "    display_tree.ax1=plt.subplot(111,frameon=False,**axprops) #create a subplot without the x y axesa and without ticks \n",
    "    draw_tree.total_width=float(0.3*countLeaf(root))  #constant used in the draw_tree \n",
    "    draw_tree.total_depth=float(0.3*findDepth(root))  #\n",
    "    draw_tree.x=-0.5/draw_tree.total_width  # x coordinates\n",
    "    draw_tree.y=1.0  #y coordinates\n",
    "    draw_tree(root,(0.5,2.0),None)\n",
    "    plt.savefig('out.png', bbox_inches='tight')  #output the figure to a png\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "clean_rows = np.loadtxt(\"clean_dataset.txt\")\n",
    "t, d = tree_learning(clean_rows, 0)\n",
    "\n",
    "display_tree(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
