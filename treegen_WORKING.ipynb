{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#to use for testing, amend path to txt file\n",
    "#===========================================#\n",
    "\n",
    "clean_data = np.loadtxt(\"clean_dataset.txt\") \n",
    "noisy_data = np.loadtxt(\"noisy_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(roomData):\n",
    "    H=0.0\n",
    "    i=0\n",
    "    count = {1:0, 2:0, 3:0, 4:0} #init dictionary to count room instances in dataset\n",
    "    size = len(roomData)\n",
    "    while i<size:\n",
    "        if roomData[i][-1] == 1:\n",
    "            count[1]+=1\n",
    "        elif roomData[i][-1] == 2:\n",
    "            count[2]+=1\n",
    "        elif roomData[i][-1] == 3:\n",
    "            count[3]+=1\n",
    "        elif roomData[i][-1] == 4:\n",
    "            count[4]+=1\n",
    "        i+=1;\n",
    "    for i in (1,2,3,4):\n",
    "        if count[i] > 0:\n",
    "            H += ((-count[i])/size)*(log2(count[i]/size))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(All, Left, Right):\n",
    "    \n",
    "    H_SAll=entropy(All) #entropy of parent datset\n",
    "    Total=len(Left)+len(Right)\n",
    "    Remainder=((len(Left)/Total)*entropy(Left))+((len(Right)/Total)*entropy(Right)) #weighted entropies of left and right datasets\n",
    "    Gains=H_SAll-Remainder\n",
    "    \n",
    "    return Gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIND_SPLIT(dataset):\n",
    "    emitter = 0\n",
    "    value = 0\n",
    "    max_info_gain = 0\n",
    "    for x in range(len(dataset[0])-1): #first 7 entries, exclude room\n",
    "        \n",
    "        ds = np.array(sorted(dataset, key = lambda y: y[x], reverse=True)) #sort n dimensional array by emitter\n",
    "        \n",
    "        for r in range(len(ds)): #iterate over every value for each emitter\n",
    "            \n",
    "            split_point = ds[r][x] #current point to check\n",
    "            if(r!=len(ds)-1 and ds[r+1][x] == split_point): #check for repeated values, use last instance of value as split point\n",
    "                continue\n",
    "            info_gain = InfoGain(ds, ds[:r+1],ds[r+1:]) #calculte information gain for current point\n",
    "            \n",
    "            if(info_gain > max_info_gain): #if information gain is higher, update the best split point and emitter\n",
    "                emitter = x\n",
    "                value = split_point\n",
    "                max_info_gain = info_gain\n",
    "            \n",
    "            \n",
    "        \n",
    "    return emitter, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(clean_rows):\n",
    "    Left = []\n",
    "    Right = []\n",
    "    em, val = FIND_SPLIT(clean_rows) #return the emitter and value for best split\n",
    "    \n",
    "    for i in range(len(clean_rows)): #iterate over dataset, using best split point, seperate dataset into left and right\n",
    "        if(clean_rows[i][em] >= val):  \n",
    "            Left.append(clean_rows[i])\n",
    "        \n",
    "        else:\n",
    "            Right.append(clean_rows[i])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return np.array(Left),np.array(Right),em,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_learning(training_data, depth):\n",
    "    \n",
    "    if(entropy(training_data) == 0 and len(training_data) != 0): #if all room labels are the same, return leaf which contains room number\n",
    "        count = len(training_data) #counting the number of entries in the dataset where the room labels are all the same (used later for pruning purposes)\n",
    "        label = training_data[0][-1]\n",
    "        leaf = {'emitter':-1, 'value':-1, 'room': label, 'right':-1, 'left':-1, 'count': count}\n",
    "        return leaf, depth\n",
    "    \n",
    "    else: \n",
    "        ldata, rdata, em, val = split(training_data) #split data to get the child datasets and split point\n",
    "        root = {'emitter': em, 'value': val, 'room': -1, 'right':-1, 'left':-1, 'count': -1} #create non leaf node\n",
    "        root['left'], l_depth = tree_learning(ldata, depth+1) #recursively call tree learning to generate rest of tree, increasing depth\n",
    "        root['right'], r_depth = tree_learning(rdata, depth+1)\n",
    "        return root, max(l_depth, r_depth) #return root of tree and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#for testing tree generation only\n",
    "#===========================================#\n",
    "tree_root, depth = tree_learning(noisy_data, 0)\n",
    "#print(tree_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_result(row, root):\n",
    "    \n",
    "    if(root['room']!= -1): #check for leaf node, return room\n",
    "        return root['room']\n",
    "    else:\n",
    "        if(row[root['emitter']]>=root['value']): #traverse tree to find leaf node\n",
    "            return return_result(row, root['left'])\n",
    "        else:\n",
    "            return return_result(row, root['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(results_table): \n",
    "    \n",
    "    cm = np.zeros((4,4)) #empty confusion matrix\n",
    "    \n",
    "    for i in range (1,5):\n",
    "        for pair in results_table: #pair[0] is actual value, pair[1] is predicted value by decision tree\n",
    "            if(i == pair[0] and pair[0] == pair[1]): #count correct predictions and update confusion matrix\n",
    "                    cm[i-1][i-1]+=1\n",
    "            if(i == pair[0] and pair[1] != i):\n",
    "                    cm[i-1][int(pair[1])-1]+=1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cm):\n",
    "    if(np.sum(cm) == 0):\n",
    "        return 0\n",
    "    return np.trace(cm)/np.sum(cm) #returns accuracy from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#function returns BOTH confusion matrix and accuracy, different from specification which only required us to return accuracy\n",
    "#===========================================#\n",
    "\n",
    "def evaluate(test, root): \n",
    "    \n",
    "    results_table = []\n",
    "    \n",
    "    for row in test: #iterate over test data and generate prediction\n",
    "        prediction = return_result(row, root)\n",
    "        results_table.append([row[-1],prediction]) #keep track of actual outcome vs predicted outcome\n",
    "        \n",
    "    cm = confusion_matrix(results_table) #generate confusion matrix from results table\n",
    "    tree_accuracy = accuracy(cm) #calculate accuracy from confusion matrix\n",
    "    \n",
    "    return cm, tree_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#cross validation WITHOUT pruning\n",
    "#===========================================#\n",
    "\n",
    "def cross_validation(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffles dataset\n",
    "\n",
    "    fold_size = int(len(dataset)/10) \n",
    "    running_cm = np.zeros((4,4)) #generate empty confusion matrix\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        #cycling test and training data for cross valdiation\n",
    "        test = dataset[0+fold_size*i:fold_size*(i+1)] \n",
    "        training = np.vstack((dataset[0:0+fold_size*i],dataset[fold_size*(i+1):])) \n",
    "        \n",
    "        root, depth = tree_learning(training, 0) #generate tree for each cycled training dataset\n",
    "        \n",
    "        confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "        running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "            \n",
    "    avg_cm = running_cm/10 #calculate average confusion matrix\n",
    "    \n",
    "    return accuracy(avg_cm) #return average accuracy\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n",
      "Runtime:  73.30419969558716\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------#\n",
    "testing = np.loadtxt(\"clean_dataset.txt\")\n",
    "st=time.time()\n",
    "print(cross_validation(testing))\n",
    "et=time.time()\n",
    "print('Runtime: ', et-st)\n",
    "#-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(cm): \n",
    "    class_precisions = []\n",
    "    cols = cm.sum(axis=0) \n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        class_precisions.append(cm[i][i]/cols[i]) #calculate precision for each class\n",
    "    \n",
    "    return class_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm):\n",
    "    class_recalls = []\n",
    "    rows = cm.sum(axis=1)\n",
    "   \n",
    "    for i in range(len(cm)):\n",
    "        class_recalls.append(cm[i][i]/rows[i]) #calculate recall for each class\n",
    "    \n",
    "    return class_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(recall, precision):\n",
    "    class_f1s = []\n",
    "    for i in range(len(recall)):\n",
    "        class_f1s.append(2*recall[i]*precision[i]/recall[i]+precision[i]) #calculate f1 measure for each class using precision and recall values\n",
    "    \n",
    "    return class_f1s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(root, validation_set, start):    \n",
    "    \n",
    "    if ((root['left']['emitter'] == -1) and (root['right']['emitter'] == -1)): #if both children leaf nodes\n",
    "        \n",
    "        old = [root['emitter'], root['value'], root['room'], root['right'], root['left']] #store all parent and children before pruning\n",
    "\n",
    "        cm, old_accuracy = evaluate(validation_set, start) #obtain accuracy of tree before prune\n",
    "        \n",
    "        \n",
    "        if(root['left']['count'] > root['right']['count']): #choose majority class label using count value\n",
    "            root['room'] = root['left']['room']\n",
    "        else:\n",
    "            root['room'] = root['right']['room']\n",
    "\n",
    "        #make parent node into single leaf node\n",
    "        root['left'] = -1 \n",
    "        root['right'] = -1\n",
    "        root['emitter'] = -1\n",
    "        root['value'] = -1\n",
    "        \n",
    "        \n",
    "        cm, new_accuracy = evaluate(validation_set, start) #obtain accuracy of tree after pruning\n",
    "        \n",
    "        if new_accuracy < old_accuracy: #if pre-pruned accuracy was better, undo prune\n",
    "            root['emitter'] = old[0]\n",
    "            root['value'] = old[1]\n",
    "            root['room'] = old[2]\n",
    "            root['right'] = old[3]\n",
    "            root['left'] = old[4]\n",
    "        \n",
    "               \n",
    "    else:\n",
    "        \n",
    "        #traverse tree to find prunable nodes\n",
    "        if(root['left']['emitter'] != -1): \n",
    "            start = prune(root['left'], validation_set, start)\n",
    "        if(root['right']['emitter'] != -1):\n",
    "            start = prune(root['right'], validation_set, start)\n",
    "    \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#cross validation WITH pruning\n",
    "#===========================================#\n",
    "\n",
    "def cross_validation_prune(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffle dataset\n",
    "    fold_size = int(len(dataset)/10)\n",
    "    \n",
    "    #initialise confusion matrices for pre and post pruned trees\n",
    "    running_cm = np.zeros((4,4))\n",
    "    running_cm_pr = np.zeros((4,4))\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        #cycle test, training and validation sets\n",
    "        test = dataset[fold_size*i:fold_size*(i+1)]     \n",
    "        validation = dataset[fold_size*(i+1)+1:fold_size*(i+2)+1]\n",
    "        training = np.vstack((dataset[0:fold_size*i],dataset[fold_size*(i+2)+1:]))\n",
    "        \n",
    "        #generate decision tree\n",
    "        root, depth = tree_learning(training, 0)\n",
    "                \n",
    "        confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "        running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "              \n",
    "        pruned_tree = prune(root, validation, root) #generate pruned tree\n",
    "#       print('Depth after:', findDepth(pruned_tree))\n",
    "        \n",
    "        cm_prune, pr_acc = evaluate(test, pruned_tree) #generate confusion matrix for current pruned tree \n",
    "        running_cm_pr += cm_prune #keep track of confusion matrices to calculate average for pruned tree\n",
    "        \n",
    "#       print(depth, \" vs \", findDepth(pruned_tree))\n",
    "#       print(tree_accuracy, \" vs \", pr_acc)\n",
    "\n",
    "    \n",
    "    avg_cm = running_cm/10\n",
    "    avg_cm_pr = running_cm_pr/10\n",
    "    \n",
    "#   print(\"Before prune accuracy:\", accuracy(avg_cm))\n",
    "#   print(\"After prune accuracy:\", accuracy(avg_cm_pr))\n",
    "    \n",
    "    return accuracy(avg_cm), accuracy(avg_cm_pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#nested cross validation WITH pruning\n",
    "#===========================================#\n",
    "\n",
    "def nested_cross_validation_prune(dataset):\n",
    "\n",
    "    np.random.shuffle(dataset) #shuffle dataset\n",
    "    fold_size = int(len(dataset)/10)\n",
    "    \n",
    "    #initialise confusion matrices for pre and post pruned trees\n",
    "    running_cm = np.zeros((4,4))\n",
    "    running_cm_pr = np.zeros((4,4))\n",
    "    \n",
    "    for j in range(10):\n",
    "    \n",
    "        test = dataset[fold_size*j:fold_size*(j+1)]\n",
    "        traning_validation_dataset = np.vstack((dataset[0:fold_size*j], dataset[fold_size*(j+1):]))\n",
    "        \n",
    "        for i in range(9):\n",
    "        \n",
    "            #cycle test, training and validation sets\n",
    "            \n",
    "            validation = traning_validation_dataset[fold_size*(i+1)+1:fold_size*(i+2)+1]\n",
    "            training = np.vstack((traning_validation_dataset[0:fold_size*i],traning_validation_dataset[fold_size*(i+2)+1:]))\n",
    "        \n",
    "            #generate decision tree\n",
    "            root, depth = tree_learning(training, 0)\n",
    "                \n",
    "            confusion_matrix, tree_accuracy = evaluate(test, root) #generate confusion matrix for current tree   \n",
    "            running_cm += confusion_matrix #keep track of confusion matrices to calculate average\n",
    "                  \n",
    "            pruned_tree = prune(root, validation, root) #generate pruned tree\n",
    "            #print('Depth after:', findDepth(pruned_tree))\n",
    "        \n",
    "            cm_prune, pr_acc = evaluate(test, pruned_tree) #generate confusion matrix for current pruned tree \n",
    "            running_cm_pr += cm_prune #keep track of confusion matrices to calculate average for pruned tree\n",
    "        \n",
    "            #print(depth, \" vs \", findDepth(pruned_tree))\n",
    "            #print(tree_accuracy, \" vs \", pr_acc)\n",
    "\n",
    "    \n",
    "    avg_cm = running_cm/90\n",
    "    avg_cm_pr = running_cm_pr/90\n",
    "    \n",
    "#   print(\"Before prune accuracy:\", accuracy(avg_cm))\n",
    "#   print(\"After prune accuracy:\", accuracy(avg_cm_pr))\n",
    "    \n",
    "    return accuracy(avg_cm), accuracy(avg_cm_pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7941666666666666, 0.8166111111111111)\n",
      "Runtime:  809.2233152389526\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------#\n",
    "testing = np.loadtxt(\"noisy_dataset.txt\")\n",
    "st=time.time()\n",
    "print(nested_cross_validation_prune(testing))\n",
    "et=time.time()\n",
    "print('Runtime: ', et-st)\n",
    "#-----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDepth(node): \n",
    "    \n",
    "    lDepth = 0\n",
    "    rDepth = 0\n",
    "    \n",
    "    if node is None: \n",
    "        return 0\n",
    "  \n",
    "    else : \n",
    "\n",
    "        #find depth for each subtree\n",
    "        if(node['left']['emitter'] != -1):\n",
    "            lDepth = findDepth(node['left']) \n",
    "        if(node['right']['emitter'] != -1):\n",
    "            rDepth = findDepth(node['right']) \n",
    "  \n",
    "        #take the larger depth since looking for maximum\n",
    "        if (lDepth > rDepth): \n",
    "            return lDepth+1\n",
    "        else: \n",
    "            return rDepth+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
